{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280a2fe5",
   "metadata": {},
   "source": [
    "# Case 1: One transmitter and one receiver.\n",
    "\n",
    "(Article: Look-Ahead and Learning Approaches for Energy Harvesting Communications Systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efa0d9",
   "metadata": {},
   "source": [
    "## Step 1. Define problem, start and goal.\n",
    "\n",
    "Point-to-point communication, one source (S) and one destination (D).\n",
    "S has infinite buffer to store data, finite battery Bmax, Bi battery levels. (Cuantizada)\n",
    "\n",
    "Model parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.63095734 0.39810717 0.25118864 0.15848932 0.1\n",
      " 0.06309573 0.03981072 0.02511886 0.01584893 0.01      ]\n"
     ]
    }
   ],
   "source": [
    "# Time slotted system. Each time slot (Ts) is 1s duration. \n",
    "Tc = 1\n",
    "\n",
    "# Bandwith\n",
    "BW = 1 # MHz\n",
    "# Noise Spectral density \n",
    "sigma_n2 =4e-21 * 1e6  # W/MHz \n",
    "sigma2 = 1e-9 # W/Hz \n",
    "\n",
    "# B is the Battery capacity. B_max is the maximum capacity. \n",
    "# Nb is the number of 'slots' in B (Quantized battery).\n",
    "B_max = 12 #units\n",
    "B_levels = list(range(B_max + 1)) \n",
    "\n",
    "# Energy harvested:\n",
    "Eh = 0.05 #J\n",
    "EH_levels = np.linspace(0,0.25,6)\n",
    "energy_levels = [0, 1, 2, 3, 4, 5]     # unidades discretas\n",
    "P_levels = EH_levels   # same as energy units\n",
    "\n",
    "# Transition Energy Probability Matrix\n",
    "E_tm = np.array([\n",
    "      [0.4011, 0.3673, 0.1027, 0.0899, 0.0279, 0.0111],\n",
    "      [0.4072, 0.3441, 0.1002, 0.0973, 0.0305, 0.0207],\n",
    "      [0.3966, 0.3239, 0.1165, 0.0860, 0.0400, 0.0370],\n",
    "      [0.3796, 0.3272, 0.1158, 0.0782, 0.0514, 0.0478],\n",
    "      [0.3612, 0.3451, 0.1055, 0.0837, 0.0501, 0.0544],\n",
    "      [0.3711, 0.3341, 0.1107, 0.0801, 0.0502, 0.0538]])\n",
    "\n",
    "# Channel states:\n",
    "H_states = np.linspace(0,-20,11) # In dB. Random transition probabilities between them.\n",
    "channel_states = [0.01, 0.05, 0.1, 0.2]  # |h|^2\n",
    "H_linear = 10 ** (H_states / 10)\n",
    "\n",
    "print(H_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2607a",
   "metadata": {},
   "source": [
    "Constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5186730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The energy harvesting and channel gain are modeled as two independent Markov chains.\n",
    "def EnergyHarvested(Eprobability,actual_e,Ne): # Mirar si se usa\n",
    "    if np.random.random() < Eprobability:\n",
    "        # stays in the same energy level\n",
    "        return actual_e\n",
    "    else:\n",
    "        # Jumps to other energy level\n",
    "        return np.random.randint(Ne)\n",
    "    \n",
    "def EnergyCausalityConstraints(P_i,B_i):\n",
    "    return Tc*P_i <= B_i\n",
    "\n",
    "def PowerConstraint(P_i):\n",
    "    return P_i >= 0\n",
    "\n",
    "def BatteryConstraint(B_i):\n",
    "    return B_i >= 0\n",
    "\n",
    "def received_signal(Pi,Hi,xi): # Mirar si se usa\n",
    "    return np.sqrt(Pi)*Hi*xi + np.random.normal(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01886c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting state selected randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71aef1",
   "metadata": {},
   "source": [
    "## Step 2. Define RL Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs\n",
    "runs = 500\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "# e-greedy algorithm variables\n",
    "def explorationProbability(i):\n",
    "    return np.exp(-0.001*i)\n",
    "# Convergence based algorithm variables: \n",
    "zeta = 4\n",
    "def tao(EpisodeTotalAvailableTime):\n",
    "    return 0.8*EpisodeTotalAvailableTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a73e5",
   "metadata": {},
   "source": [
    "Reward Funcition (amount of received data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RewardFunction(power_idx, h_idx):\n",
    "    Power_i = P_levels[power_idx]\n",
    "    Hi = H_linear[h_idx]\n",
    "    return Tc*np.log2(1+(Power_i*(Hi**2))/sigma_n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab67ed",
   "metadata": {},
   "source": [
    "MDP Reformulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== VALID ACTIONS ======\n",
    "def valid_actions(b_idx):\n",
    "    battery = B_levels[b_idx]\n",
    "    return [p for p in P_levels if p <= battery]\n",
    "\n",
    "# ====== Energy and channel probability transition ======\n",
    "    # Probability matrix between energy states\n",
    "def energy_transition_prob(e_idx, e_next_idx):\n",
    "    return E_tm[e_idx, e_next_idx]\n",
    "    # Random transition probabilities between channel states.\n",
    " \n",
    "def channel_transition_prob():\n",
    "    return np.random.uniform(0, 1)\n",
    "\n",
    "def next_BatteryState(b_idx, e_idx, power_idx):\n",
    "   B_i = B_levels[b_idx]\n",
    "   E_i = EH_levels[e_idx]\n",
    "   Power_i = P_levels[power_idx]\n",
    "   B_next = min(max(B_i + E_i - Tc*Power_i,0), B_max)\n",
    "   return B_levels.index(B_next)\n",
    "\n",
    "    # Full state transition probability: \n",
    "def TransitionProbability(state, power_idx, next_state):\n",
    "    b_idx, h_idx, e_idx = state\n",
    "    b2, h2, e2 = next_state\n",
    "    # state = (b, h, e)\n",
    "    if b2 != next_BatteryState(b_idx, e_idx, power_idx):\n",
    "        return 0.0\n",
    "    \n",
    "    return energy_transition_prob(e_idx,e2)*channel_transition_prob()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [B_levels, H_linear, EH_levels]\n",
    "#s = (b_idx, h_idx, e_idx)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f2ae8",
   "metadata": {},
   "source": [
    "## Look-ahead policy for EH communications\n",
    "\n",
    "First problem: The source has causal knowledge about states (Knowledge about the past and current states).\n",
    "\n",
    "The objective is to reduce the overflow energy. Use all overflow energy regardless of the channel state, otherwise it will be lost. The paper wants to find an optimal transmission policy:\n",
    "\n",
    "Given the current battery level, channel condition, and harvested energy, how much power should I transmit to maximize long-term throughput?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overflow_energy(battery_idx,energy_idx):\n",
    "    battery = B_levels[battery_idx]\n",
    "    energy = EH_levels[energy_idx]\n",
    "    return np.max(battery+np.min(energy,B_max)-B_max,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a07a4",
   "metadata": {},
   "source": [
    "Algorithm 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_equation(state, next_state, power_idx,next_power_idx):\n",
    "    b_idx, h_idx, e_idx = state\n",
    "    reward = RewardFunction(power_idx, h_idx) + gamma*TransitionProbability(state, next_power_idx, next_state)*RewardFunction(next_power_idx, next_state[1])\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9194e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" num_states = len(B_levels) * len(H_states) * len(EH_levels)\n",
    "V = np.zeros((len(B_levels), len(H_states), len(EH_levels)))\n",
    "policy = np.zeros_like(V, dtype=int)\n",
    "for b_idx in range(len(B_levels)):\n",
    "        for h_idx in range(len(H_states)):\n",
    "            for e_idx in range(len(EH_levels)):\n",
    "\n",
    "                e_overflow = overflow_energy(b_idx,e_idx)\n",
    "\n",
    "                for power_idx in valid_actions(b_idx) \"\"\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia directa de chatgpt \n",
    "def lookahead_value(state, energy_used, P_e, P_h):\n",
    "    b, h, e = state\n",
    "    p_tx = energy_used / Tc\n",
    "    p_idx = P_levels.index(p_tx)\n",
    "    h_idx = H_linear.index(h)\n",
    "    b_idx = B_levels.index(b)\n",
    "    e_idx = B_levels.index(e)\n",
    "    \n",
    "    # Recompensa actual\n",
    "    r_now = RewardFunction(p_idx, h_idx)\n",
    "    \n",
    "    # Batería futura\n",
    "    b_next = next_BatteryState(b_idx, e_idx, p_idx)\n",
    "    \n",
    "    expected_future = 0.0\n",
    "    \n",
    "    for e_next, pe in P_e.items():\n",
    "        for h2_next, ph in P_h.items():\n",
    "            r_next = RewardFunction(b_next / Tc, h2_next)\n",
    "            expected_future += pe * ph * r_next\n",
    "    \n",
    "    return r_now + gamma * expected_future\n",
    "\n",
    "def lookahead_policy(state, P_e, P_h, M=None):\n",
    "    b, h2, e = state\n",
    "    \n",
    "    e_ovf = overflow_energy(b, e)\n",
    "    possible_energies = list(range(e_ovf, b + 1))\n",
    "    \n",
    "    if M is not None and M < len(possible_energies):\n",
    "        possible_energies = np.random.sample(possible_energies, M)\n",
    "    \n",
    "    best_val = -np.inf\n",
    "    best_energy = 0\n",
    "    \n",
    "    for lam in possible_energies:\n",
    "        val = lookahead_value(state, lam, P_e, P_h)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_energy = lam\n",
    "    \n",
    "    return best_energy / Tc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64fd77",
   "metadata": {},
   "source": [
    "## Aprendizaje por refuerzo (modelo desconocido)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890dd43",
   "metadata": {},
   "source": [
    "Estructura Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "Q = defaultdict(lambda: defaultdict(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2654e",
   "metadata": {},
   "source": [
    "SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_update(s, a, r, s_next, a_next, alpha):\n",
    "    Q[s][a] += alpha * (r + gamma * Q[s_next][a_next] - Q[s][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de7579",
   "metadata": {},
   "source": [
    "Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f530db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(s, a, r, s_next, alpha):\n",
    "    best_next = max(Q[s_next].values()) if Q[s_next] else 0\n",
    "    Q[s][a] += alpha * (r + gamma * best_next - Q[s][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbfc40",
   "metadata": {},
   "source": [
    "Exploración ε-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc38d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, actions, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    return max(actions, key=lambda a: Q[state][a])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
