{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b707ffa",
   "metadata": {},
   "source": [
    "# Power Energy Transfer System: 4 vs 1\n",
    "\n",
    "Sistema de doble aporendizaje por refuerzo, donde una matriz de antenas emite energía a una antena receptora. La posición de la antena receptora en principio es desconocida para la antena emisora. En el downlink la matriz de antenas emisoras transmitirá una cantidad determinada y siempre igual de potencia en una dirección (a decidir) y la antena receptora recibirá una cantidad de energía X según el canal (H) y la dirección del emisor. En el uplink, la antena receptora tiene que decidir si envía datos o no en base a la batería que tenga y la energía que ha captado. La antena emisora recibirá esos datos, si la receptora ha decidido enviarlos, y en base a eso recalculará la dirección. En los datos está la energía que le ha llegado al receptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71813d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Models import modelos_indoor\n",
    "from antenna_distributions import antenna_distributions as antd\n",
    "from itertools import product\n",
    "\n",
    "np.set_printoptions(legacy='1.25')\n",
    "np.random.seed(2026)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da13b0",
   "metadata": {},
   "source": [
    "1. Parámetros físicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73adec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 3e8\n",
    "freq = 5.8e9    \n",
    "lam = c / freq\n",
    "k = 2 * np.pi / lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a55ee5",
   "metadata": {},
   "source": [
    "2. Parámetros de la habitación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35dfc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "room = [10.0,10.0,3.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192c58f",
   "metadata": {},
   "source": [
    "3. Parámetros antena emisora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8820a9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.01293103448275862, -0.01293103448275862, 3.0), (-0.01293103448275862, 0.01293103448275862, 3.0), (0.01293103448275862, -0.01293103448275862, 3.0), (0.01293103448275862, 0.01293103448275862, 3.0)]\n",
      "81\n",
      "[1.3781892705668197, 2.5950292768487633, 6.136381686847114, 0.5585690266421656]\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "num_ant = 2*2\n",
    "Ptt = 100.0\n",
    "Pt_i = Ptt/num_ant \n",
    "# La antena transmisora es isotrópica.\n",
    "# Ganancia\n",
    "G_t = 1.0\n",
    "\n",
    "# Dos posiciones de antenas emisoras (Lambda/2 y distribuidas): \n",
    "d1 = lam / 2 \n",
    "d2 = room[0] / np.sqrt(num_ant)\n",
    "\n",
    "p_lamda2 = antd.grid_antenna_positions(num_ant,d1,room[2])\n",
    "print(p_lamda2)\n",
    "p_dist = antd.grid_antenna_positions(num_ant,d2,room[2])\n",
    "\n",
    "# Ángulos a los que puede variar la fase cada una de las antenas emisoras.\n",
    "# Puede sumar 0.1, restar 0.1 o quedarse donde está.\n",
    "# cada uno de las antenas emisoras. 3 estados por antena.\n",
    "angle_resolution = 0.1\n",
    "emisor_possible_actions = 3**num_ant\n",
    "\n",
    "print(emisor_possible_actions)\n",
    "\n",
    "# ESTADO INICIAL:\n",
    "initial_angles_emisor = []\n",
    "np.random.seed(2026)\n",
    "for i in range(num_ant): initial_angles_emisor.append(np.random.uniform(0, 2*np.pi))\n",
    "print(initial_angles_emisor)\n",
    "\n",
    "def permutaciones_estados(N, estados=(-0.1, 0, 0.1)):\n",
    "    return list(product(estados, repeat=N))\n",
    "\n",
    "actions_transmitter = permutaciones_estados(num_ant,(-0.1, 0, 0.1))\n",
    "print(len(actions_transmitter))\n",
    "\n",
    "# El estado depende del ángulo, y la información recibida. \n",
    "# STATE = [RESPONSE]\n",
    "# REWARD = RESPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a641ffad",
   "metadata": {},
   "source": [
    "4. Parámetros de la antena receptora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ec9610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 4, 'P_total': 100.0, 'f': 5800000000.0, 'G_t': 1.0, 'G_r': 1.0}\n",
      "1000\n",
      "100\n",
      "11\n",
      "[1.3781892705668197, 2.5950292768487633, 6.136381686847114, 0.5585690266421656]\n",
      "0.0006077702243128578 0.6818259436700033\n",
      "(6, 257, 0)\n"
     ]
    }
   ],
   "source": [
    "# Dipolo vertical:\n",
    "def theta_r(_tx,_rx):\n",
    "    R_path = np.linalg.norm(_rx - _tx)\n",
    "    cos_theta = abs((_rx - _tx))[2] / (R_path + 1e-15)\n",
    "    sin_theta = np.sqrt(max(0.0, 1.0 - cos_theta**2))\n",
    "    return sin_theta**2\n",
    "\n",
    "# Posición de la antena receptora: \n",
    "r_position = np.array([3, 0, 1]) \n",
    "# Ganancia: \n",
    "G_r = 1.0\n",
    "\n",
    "params = {'n':num_ant, 'P_total': Ptt, 'f': freq, 'G_t': G_t, 'G_r': G_r,}\n",
    "print(params)\n",
    "\n",
    "# Unidades en J/s de cada nivel de la batería (1W=1J/s). \n",
    "Eh = 0.005\n",
    "B_max = 5\n",
    "B_levels = np.arange(0,B_max,Eh)\n",
    "print(len(B_levels))\n",
    "actions = B_levels\n",
    "\n",
    "Eh_max = 0.5\n",
    "Eh_levels = np.arange(0,Eh_max,Eh)\n",
    "print(len(Eh_levels))\n",
    "\n",
    "H_levels = np.arange(0,1.1,0.1)\n",
    "print(len(H_levels))\n",
    "\n",
    "# Acciones posibles son tres: 0. No enviar nada. 1. Enviar 'baliza de estado'. 2. Captar información. 3. Enviar información.\n",
    "action_0 = 0\n",
    "action_1 = 0.000001\n",
    "action_2 = 0.00001\n",
    "action_3 = 0.0005\n",
    "actions_receiver = [action_0,action_1,action_2, action_3]\n",
    "\n",
    "# El estado depende de H, el nivel de batería y la energía captada.\n",
    "# STATE = [H,B,EH]\n",
    "\n",
    "# ESTADO INICIAL\n",
    "np.random.seed(2026)\n",
    "B_init = (np.random.choice(B_levels))\n",
    "print(initial_angles_emisor)\n",
    "power_received, probability = modelos_indoor.total_power_shadowing(p_lamda2,initial_angles_emisor,r_position,params)\n",
    "print(power_received, probability)\n",
    "\n",
    "# To quantize the values. \n",
    "def get_index(value,from_array):\n",
    "    from_list = list(from_array)\n",
    "    #print(from_list)\n",
    "    the_index = len(from_list)-1\n",
    "    for i in from_list: \n",
    "        if i > value:\n",
    "            the_index = from_list.index(i)-1\n",
    "            break\n",
    "    return the_index\n",
    "\n",
    "receiver_init_state = (get_index(probability,H_levels), list(B_levels).index(B_init), get_index(power_received,Eh_levels))\n",
    "initial_emisor_state = (get_index(power_received,Eh_levels))\n",
    "print(receiver_init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217697e",
   "metadata": {},
   "source": [
    "5. Parámetros Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e20de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 81)\n",
      "(11, 1000, 100, 4)\n"
     ]
    }
   ],
   "source": [
    "# num_episodes: Number of times the agent will attempt to navigate the maze.\n",
    "num_episodes = 1000\n",
    "\n",
    "# Tiempo por episodio\n",
    "total_time=2000\n",
    "\n",
    "# alpha: Learning rate that controls how much new information overrides old information.\n",
    "alpha = 0.1\n",
    "# gamma: Discount factor giving more weight to immediate rewards.\n",
    "gamma = 0.9\n",
    "\n",
    "# Q is the Q-Table initialized to zero; it stores expected rewards for each state-action pair.\n",
    "# Hay 2 matrices de Q, para el emisor y el receptor.\n",
    "\n",
    "Q_emisor = np.zeros((len(Eh_levels),)+(len(actions_transmitter),))\n",
    "print(Q_emisor.shape)\n",
    "\n",
    "Q_receiver = np.zeros((len(H_levels),)+(len(B_levels),)+(len(Eh_levels),)+(len(actions_receiver),))\n",
    "print(Q_receiver.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797abc4",
   "metadata": {},
   "source": [
    "6. Acciones válidas para ambos emisores y receptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e1331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como mínimo, se envía la energía en overflow, si se da la situación.\n",
    "\n",
    "def valid_energy_transfer(state): # No se da ni de locura\n",
    "    E_overflow = max(state[2]+min(state[1],B_max)-B_max,0)\n",
    "    available_actions = state[0]\n",
    "    valid_actions = [\n",
    "        a for a in actions\n",
    "        if E_overflow <= a <= available_actions\n",
    "    ]\n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b68537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve el valor de valid_actions (índices) que es máximo en Q[state].\n",
    "def get_Q_max_action(valid_actions,state,Q):\n",
    "    return int(max(valid_actions, key=lambda a: Q[state][a]))\n",
    "\n",
    "def explotation_vs_exploration(valid_actions,state, epsilon,Q): #receptor\n",
    "    if np.random.random() < epsilon:\n",
    "        #Exploration (Probability e)\n",
    "        return int(np.random.choice(valid_actions))\n",
    "    else: \n",
    "        #Explotation (Probability 1-e)\n",
    "        return int(get_Q_max_action(valid_actions, state,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856e1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reward_emisor(power_received):\n",
    "    return power_received*100\n",
    "\n",
    "def Reward_receiver(action_used, informacion_captada,H_prob): #?\n",
    "    if(action_used == 0):\n",
    "        return 0*H_prob\n",
    "    elif(action_used == 1):\n",
    "        return 1*H_prob\n",
    "    elif(action_used == 2):\n",
    "        return 1*H_prob\n",
    "    elif(action_used == 3):\n",
    "        return min(informacion_captada, 10)*H_prob\n",
    "    else: \n",
    "        return print(\"Error. Acción fuera de rango.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47362d73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m action_transmitter_index = explotation_vs_exploration(valid_actions_idx_transmitter,state_transmitter,epsilon, Q_emisor)\n\u001b[32m     48\u001b[39m angles_transmitter += actions_transmitter[action_transmitter_index]\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m power_received, probability = \u001b[43mmodelos_indoor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtotal_power_shadowing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_lamda2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mangles_transmitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# el receiver se actualiza aquí, según el nuevo ángulo del transmitter. \u001b[39;00m\n\u001b[32m     52\u001b[39m next_H_idx = get_index(probability,H_levels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asociado\\Documents\\Reinforcement Learning\\Models.py:185\u001b[39m, in \u001b[36mmodelos_indoor.total_power_shadowing\u001b[39m\u001b[34m(tx_positions, tx_theta, r_position, params)\u001b[39m\n\u001b[32m    183\u001b[39m prob = \u001b[32m0\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tx,tx_phase  \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tx_positions,tx_theta):\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     E, p = \u001b[43mmodelos_indoor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msingle_antenna_received_power_shadowing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx_phase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     E_total += E\n\u001b[32m    187\u001b[39m     prob += p\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asociado\\Documents\\Reinforcement Learning\\Models.py:161\u001b[39m, in \u001b[36mmodelos_indoor.single_antenna_received_power_shadowing\u001b[39m\u001b[34m(tx, tx_phase, r_position, params)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingle_antenna_received_power_shadowing\u001b[39m(tx, tx_phase, r_position,params):\n\u001b[32m    160\u001b[39m     R_path = np.linalg.norm(np.array(r_position)  - np.array(tx))\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     dir_gain = \u001b[43mmodelos_indoor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtheta_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr_position\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     Pt_i = params[\u001b[33m\"\u001b[39m\u001b[33mP_total\u001b[39m\u001b[33m\"\u001b[39m] / params[\u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    163\u001b[39m     lam = modelos_indoor.c / params[\u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asociado\\Documents\\Reinforcement Learning\\Models.py:16\u001b[39m, in \u001b[36mmodelos_indoor.theta_r\u001b[39m\u001b[34m(_tx, _rx)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtheta_r\u001b[39m(_tx,_rx):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     R_path = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_rx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43m_tx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     cos_theta = \u001b[38;5;28mabs\u001b[39m((_rx - _tx))[\u001b[32m2\u001b[39m] / (R_path + \u001b[32m1e-15\u001b[39m)\n\u001b[32m     18\u001b[39m     sin_theta = np.sqrt(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m - cos_theta**\u001b[32m2\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asociado\\Documents\\Reinforcement Learning\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2792\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2790\u001b[39m     sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n\u001b[32m   2791\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2792\u001b[39m     sqnorm = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2793\u001b[39m ret = sqrt(sqnorm)\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "reward_transmitter_all_episodes = []\n",
    "reward_receiver_all_episodes = []\n",
    "\n",
    "valid_actions_idx_receiver = list(range(len(actions_receiver)))\n",
    "valid_actions_idx_transmitter = list(range(len(actions_transmitter)))\n",
    "\n",
    "epsilon = 0.2\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    total_rewards_transmitter = 0\n",
    "    total_rewards_receiver = 0\n",
    "    state_transmitter = int(initial_emisor_state)\n",
    "    state_receiver = tuple(int(x) for x in receiver_init_state)\n",
    "    informacion_captada = 0\n",
    "    baliza_enviada = 0\n",
    "    angles_transmitter = initial_angles_emisor\n",
    "    for time in range(total_time):\n",
    "        if time % 2 == 0:\n",
    "            #receiver\n",
    "            # STATE = [H,B,EH]\n",
    "            H_idx, B_idx,  EH_idx = state_receiver\n",
    "            \"\"\" H_idx = list(H_levels).index(H_prob)\n",
    "            B_idx = list(B_levels).index(B_state)\n",
    "            EH_idx = list(Eh_levels).index(EH_state) \"\"\"\n",
    "            \n",
    "            action_receiver_index = explotation_vs_exploration(valid_actions_idx_receiver,state_receiver,epsilon, Q_receiver)\n",
    "            if action_receiver_index == 2:\n",
    "                informacion_captada += 1\n",
    "                baliza_enviada = 0\n",
    "            elif action_receiver_index == 1:\n",
    "                baliza_enviada = Eh_levels[EH_idx]\n",
    "            else: \n",
    "                baliza_enviada = 0\n",
    "            \n",
    "            reward_receiver = Reward_receiver(action_receiver_index,informacion_captada, H_levels[H_idx])\n",
    "            next_B = min(B_levels[B_idx] + Eh_levels[EH_idx] - actions_receiver[action_receiver_index], B_max)\n",
    "            next_B_idx = get_index(next_B,B_levels)\n",
    "\n",
    "            old_value_receiver = Q_receiver[H_idx,B_idx, EH_idx][action_receiver_index]\n",
    "            total_rewards_receiver += reward_receiver\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # transmitter\n",
    "            reward_transmitter = Reward_emisor(baliza_enviada)\n",
    "\n",
    "            action_transmitter_index = explotation_vs_exploration(valid_actions_idx_transmitter,state_transmitter,epsilon, Q_emisor)\n",
    "            angles_transmitter += actions_transmitter[action_transmitter_index]\n",
    "            power_received, probability = modelos_indoor.total_power_shadowing(p_lamda2,angles_transmitter,r_position,params)\n",
    "            \n",
    "            # el receiver se actualiza aquí, según el nuevo ángulo del transmitter. \n",
    "            next_H_idx = get_index(probability,H_levels)\n",
    "            next_EH_idx = get_index(power_received,Eh_levels)\n",
    "            next_receiver_state = (next_H_idx,next_B_idx, next_EH_idx)\n",
    "            next_receiver_max = max(Q_receiver[next_receiver_state])\n",
    "            Q_receiver[state_receiver][action_receiver_index] += alpha*(reward_receiver + gamma*next_receiver_max - old_value_receiver)\n",
    "            state_receiver = next_receiver_state\n",
    "            \n",
    "            # teniendo nuevo ángulo en el transmisor:\n",
    "            old_value_transmitter = Q_emisor[state_transmitter][action_transmitter_index]\n",
    "            next_state_transmitter = next_EH_idx\n",
    "            next_max_transmitter = max(Q_emisor[next_state_transmitter])\n",
    "            Q_emisor[state_transmitter][action_transmitter_index] += alpha*(reward_transmitter + gamma*next_max_transmitter - old_value_transmitter)\n",
    "            state_transmitter = next_state_transmitter\n",
    "            total_rewards_transmitter += reward_transmitter\n",
    "\n",
    "\n",
    "    reward_transmitter_all_episodes.append(total_rewards_transmitter)\n",
    "    reward_receiver_all_episodes.append(total_rewards_receiver)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(reward_transmitter_all_episodes)), reward_transmitter_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_episodes), reward_receiver_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward_receiver_all_episodes)\n",
    "plt.plot(range(len(reward_transmitter_all_episodes)), reward_transmitter_all_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
